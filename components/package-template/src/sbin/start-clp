#!/usr/bin/env python3
import argparse
import logging
import multiprocessing
import os
import pathlib
import socket
import subprocess
import sys
import time
import uuid

# Setup logging
# Create logger
logger = logging.getLogger('clp')
logger.setLevel(logging.INFO)
# Setup console logging
logging_console_handler = logging.StreamHandler()
logging_formatter = logging.Formatter("%(asctime)s [%(levelname)s] [%(name)s] %(message)s")
logging_console_handler.setFormatter(logging_formatter)
logger.addHandler(logging_console_handler)


def get_clp_home():
    # Determine CLP_HOME from an environment variable or this script's path
    _clp_home = None
    if 'CLP_HOME' in os.environ:
        _clp_home = pathlib.Path(os.environ['CLP_HOME'])
    else:
        for path in pathlib.Path(__file__).resolve().parents:
            if 'sbin' == path.name:
                _clp_home = path.parent
                break

    if _clp_home is None:
        logger.error("CLP_HOME is not set and could not be determined automatically.")
        return None
    elif not _clp_home.exists():
        logger.error("CLP_HOME set to nonexistent path.")
        return None

    return _clp_home.resolve()


def load_bundled_python_lib_path(_clp_home):
    python_site_packages_path = _clp_home / 'lib' / 'python3' / 'site-packages'
    if not python_site_packages_path.is_dir():
        logger.error("Failed to load python3 packages bundled with CLP.")
        return False

    # Add packages to the front of the path
    sys.path.insert(0, str(python_site_packages_path))

    return True


clp_home = get_clp_home()
if clp_home is None or not load_bundled_python_lib_path(clp_home):
    sys.exit(-1)

import yaml
from clp.package_utils import \
    CLP_DEFAULT_CONFIG_FILE_RELATIVE_PATH, \
    CONTAINER_CLP_HOME, \
    check_dependencies, \
    container_exists, \
    CLPDockerMounts, \
    DockerMount, \
    DockerMountType, \
    generate_container_config, \
    validate_and_load_config_file, \
    validate_and_load_db_credentials_file, \
    validate_and_load_compression_queue_credentials_file, \
    validate_db_config, \
    validate_results_cache_config, \
    validate_worker_config, validate_queue_config, \
    validate_webui_config
from clp_py_utils.clp_config import (
    CLPConfig,
    DB_COMPONENT_NAME,
    COMPRESSION_QUEUE_COMPONENT_NAME,
    Queue,
    RESULTS_CACHE_COMPONENT_NAME,
    SEARCH_QUEUE_COMPONENT_NAME,
    SEARCH_SCHEDULER_COMPONENT_NAME,
    SEARCH_WORKER_COMPONENT_NAME,
    COMPRESSION_WORKER_COMPONENT_NAME,
    COMPRESSION_JOB_HANDLER_COMPONENT_NAME,
    WEBUI_COMPONENT_NAME,
)
from job_orchestration.scheduler.constants import QueueName


def append_docker_port_settings_for_host_ips(hostname: str, host_port: int, container_port: int, cmd: [str]):
    # Note: We use a set because gethostbyname_ex can return the same IP twice for one hostname
    for ip in set(socket.gethostbyname_ex(hostname)[2]):
        cmd.append('-p')
        cmd.append(f'{ip}:{host_port}:{container_port}')


def wait_for_database_to_init(container_name: str, clp_config: CLPConfig, timeout: int):
    # Try to connect to the database
    begin_time = time.time()
    container_exec_cmd = [
        'docker', 'exec',
        '-it',
        container_name
    ]
    mysqladmin_cmd = [
        'mysqladmin', 'ping',
        '--silent',
        '-h', '127.0.0.1',
        '-u', str(clp_config.database.username),
        f'--password={clp_config.database.password}'
    ]
    cmd = container_exec_cmd + mysqladmin_cmd
    while True:
        try:
            subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)
            return True
        except subprocess.CalledProcessError:
            if time.time() - begin_time > timeout:
                break
            time.sleep(1)

    logger.error(f"Timeout while waiting for {DB_COMPONENT_NAME} to initialize.")
    return False


def start_db(instance_id: str, clp_config: CLPConfig, conf_dir: pathlib.Path):
    logger.info(f"Starting {DB_COMPONENT_NAME}...")

    container_name = f'clp-{DB_COMPONENT_NAME}-{instance_id}'
    if container_exists(container_name):
        logger.info(f"{DB_COMPONENT_NAME} already running.")
        return

    db_data_dir = clp_config.data_directory / DB_COMPONENT_NAME
    db_logs_dir = clp_config.logs_directory / DB_COMPONENT_NAME

    validate_db_config(clp_config, db_data_dir, db_logs_dir)

    # Create directories
    db_data_dir.mkdir(exist_ok=True, parents=True)
    db_logs_dir.mkdir(exist_ok=True, parents=True)

    # Start container
    mounts = [
        DockerMount(DockerMountType.BIND, conf_dir / 'mysql' / 'conf.d', pathlib.Path('/') / 'etc' / 'mysql' / 'conf.d',
                    True),
        DockerMount(DockerMountType.BIND, db_data_dir, pathlib.Path('/') / 'var' / 'lib' / 'mysql'),
        DockerMount(DockerMountType.BIND, db_logs_dir, pathlib.Path('/') / 'var' / 'log' / 'mysql'),
    ]
    cmd = [
        'docker', 'run',
        '-d',
        '--rm',
        '--name', container_name,
        '-e', f'MYSQL_ROOT_PASSWORD={clp_config.database.password}',
        '-e', f'MYSQL_USER={clp_config.database.username}',
        '-e', f'MYSQL_PASSWORD={clp_config.database.password}',
        '-e', f'MYSQL_DATABASE={clp_config.database.name}',
        '-u', f'{os.getuid()}:{os.getgid()}',
    ]
    for mount in mounts:
        cmd.append('--mount')
        cmd.append(str(mount))
    append_docker_port_settings_for_host_ips(clp_config.database.host, clp_config.database.port, 3306, cmd)
    if 'mysql' == clp_config.database.type:
        cmd.append('mysql:8.0.23')
    elif 'mariadb' == clp_config.database.type:
        cmd.append('mariadb:10.6.4-focal')
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)

    if not wait_for_database_to_init(container_name, clp_config, 150):
        raise EnvironmentError(f"{DB_COMPONENT_NAME} did not initialize in time")

    logger.info(f"Started {DB_COMPONENT_NAME}.")


def create_db_tables(instance_id: str, clp_config: CLPConfig, container_clp_config: CLPConfig, mounts: CLPDockerMounts):
    logger.info(f"Creating {DB_COMPONENT_NAME} tables...")

    container_name = f'clp-{DB_COMPONENT_NAME}-table-creator-{instance_id}'

    # Create database config file
    db_config_filename = f'{container_name}.yml'
    db_config_file_path = clp_config.logs_directory / db_config_filename
    with open(db_config_file_path, 'w') as f:
        yaml.safe_dump(container_clp_config.database.dict(), f)

    clp_site_packages_dir = CONTAINER_CLP_HOME / 'lib' / 'python3' / 'site-packages'
    container_start_cmd = [
        'docker', 'run',
        '-i',
        '--network', 'host',
        '--rm',
        '--name', container_name,
        '-e', f'PYTHONPATH={clp_site_packages_dir}',
        '-u', f'{os.getuid()}:{os.getgid()}',
        '--mount', str(mounts.clp_home),
    ]
    necessary_mounts = [mounts.data_dir, mounts.logs_dir]
    for mount in necessary_mounts:
        if mount:
            container_start_cmd.append('--mount')
            container_start_cmd.append(str(mount))
    container_start_cmd.append(clp_config.execution_container)

    clp_py_utils_dir = clp_site_packages_dir / 'clp_py_utils'
    create_tables_cmd = [
        'python3',
        str(clp_py_utils_dir / 'create-db-tables.py'),
        '--config', str(container_clp_config.logs_directory / db_config_filename),
    ]

    cmd = container_start_cmd + create_tables_cmd
    logger.debug(' '.join(cmd))
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)

    db_config_file_path.unlink()

    logger.info(f"Created {DB_COMPONENT_NAME} tables.")


def start_compression_queue(instance_id: str, clp_config: CLPConfig):
    component_name = COMPRESSION_QUEUE_COMPONENT_NAME
    queue_logs_dir = clp_config.logs_directory / component_name
    start_queue(component_name, instance_id, clp_config.compression_queue, queue_logs_dir)


def start_search_queue(instance_id: str, clp_config: CLPConfig):
    component_name = SEARCH_QUEUE_COMPONENT_NAME
    queue_logs_dir = clp_config.logs_directory / component_name
    start_queue(component_name, instance_id, clp_config.search_queue, queue_logs_dir)


def start_queue(component_name: str, instance_id: str, queue_config: Queue,
                queue_logs_dir: pathlib.Path):
    logger.info(f"Starting {component_name}...")

    container_name = f'clp-{component_name}-{instance_id}'
    if container_exists(container_name):
        logger.info(f"{component_name} already running.")
        return

    validate_queue_config(component_name, queue_config, queue_logs_dir)

    # Create directories
    queue_logs_dir.mkdir(exist_ok=True, parents=True)

    log_filename = 'rabbitmq.log'

    # Generate config file
    config_filename = f'{container_name}.conf'
    host_config_file_path = queue_logs_dir / config_filename
    with open(host_config_file_path, 'w') as f:
        f.write(f"default_user = {queue_config.username}\n")
        f.write(f"default_pass = {queue_config.password}\n")
        f.write(f"log.file = {log_filename}\n")

    # Start container
    rabbitmq_logs_dir = pathlib.Path('/') / 'var' / 'log' / 'rabbitmq'
    mounts = [
        DockerMount(DockerMountType.BIND, host_config_file_path,
                    pathlib.Path('/') / 'etc' / 'rabbitmq' / 'rabbitmq.conf', True),
        DockerMount(DockerMountType.BIND, queue_logs_dir, rabbitmq_logs_dir),
    ]
    rabbitmq_pid_file_path = pathlib.Path('/') / 'tmp' / 'rabbitmq.pid'
    cmd = [
        'docker', 'run',
        '-d',
        '--rm',
        '--name', container_name,
        # Override RABBITMQ_LOGS since the image sets it to *only* log to stdout
        '-e', f'RABBITMQ_LOGS={rabbitmq_logs_dir / log_filename}',
        '-e', f'RABBITMQ_PID_FILE={rabbitmq_pid_file_path}',
        '-u', f'{os.getuid()}:{os.getgid()}',
    ]
    append_docker_port_settings_for_host_ips(queue_config.host, queue_config.port, 5672, cmd)
    for mount in mounts:
        cmd.append('--mount')
        cmd.append(str(mount))
    cmd.append('rabbitmq:3.9.8')
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)

    # Wait for queue to start up
    cmd = [
        'docker', 'exec', '-it', container_name,
        'rabbitmqctl', 'wait', str(rabbitmq_pid_file_path),
    ]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)

    logger.info(f"Started {component_name}.")


def start_results_cache(instance_id: str, clp_config: CLPConfig, conf_dir: pathlib.Path):
    component_name = RESULTS_CACHE_COMPONENT_NAME

    logger.info(f"Starting {component_name}...")

    container_name = f'clp-{component_name}-{instance_id}'
    if container_exists(container_name):
        logger.info(f"{component_name} already running.")
        return

    data_dir = clp_config.data_directory / component_name
    logs_dir = clp_config.logs_directory / component_name

    validate_results_cache_config(clp_config, data_dir, logs_dir)

    # Create directories
    data_dir.mkdir(exist_ok=True, parents=True)
    logs_dir.mkdir(exist_ok=True, parents=True)

    # Start container
    mounts = [
        DockerMount(DockerMountType.BIND, conf_dir / 'mongo', pathlib.Path('/') / 'etc' / 'mongo',
                    True),
        DockerMount(DockerMountType.BIND, data_dir, pathlib.Path('/') / 'data' / 'db'),
        DockerMount(DockerMountType.BIND, logs_dir, pathlib.Path('/') / 'var' / 'log' / 'mongodb'),
    ]
    cmd = [
        'docker', 'run',
        '-d',
        '--rm',
        '--name', container_name,
        '-u', f'{os.getuid()}:{os.getgid()}',
    ]
    for mount in mounts:
        cmd.append('--mount')
        cmd.append(str(mount))
    append_docker_port_settings_for_host_ips(clp_config.results_cache.host,
                                             clp_config.results_cache.port, 27017, cmd)
    cmd.append('mongo:7.0.1')
    cmd.append('--config')
    cmd.append(str(pathlib.Path('/') / 'etc' / 'mongo' / 'mongod.conf'))
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)

    logger.info(f"Started {component_name}.")


def start_compression_job_handler(instance_id: str, clp_config: CLPConfig, container_clp_config: CLPConfig,
                                  mounts: CLPDockerMounts):
    component_name = COMPRESSION_JOB_HANDLER_COMPONENT_NAME

    logger.info(f"Starting {component_name}...")

    container_name = f'clp-{component_name}-{instance_id}'
    if container_exists(container_name):
        logger.info(f"{component_name} already running.")
        return

    clp_config.archive_output.directory.mkdir(parents=True, exist_ok=True)

    clp_site_packages_dir = CONTAINER_CLP_HOME / 'lib' / 'python3' / 'site-packages'
    container_start_cmd = [
        'docker', 'run',
        '-di',
        '--network', 'host',
        '-w', str(CONTAINER_CLP_HOME),
        '--rm',
        '--name', container_name,
        '-e', f'PYTHONPATH={clp_site_packages_dir}',
        '-e', f'DB_USER={clp_config.database.username}',
        '-e', f'DB_PASSWORD={clp_config.database.password}',
        '-e', f'BROKER_URL=amqp://'
              f'{container_clp_config.compression_queue.username}:{container_clp_config.compression_queue.password}@'
              f'{container_clp_config.compression_queue.host}:{container_clp_config.compression_queue.port}',
        '-e', f'RESULT_BACKEND=rpc://'
              f'{container_clp_config.compression_queue.username}:{container_clp_config.compression_queue.password}'
              f'@{container_clp_config.compression_queue.host}:{container_clp_config.compression_queue.port}',
        # V0.5 TODO: customization for filer
        #'-u', f'{os.getuid()}:{os.getgid()}',
        '-u', f'root:root',
        '--mount', str(mounts.clp_home),
    ]
    necessary_mounts = [
        mounts.archives_output_dir,
        mounts.input_logs_dir,
        mounts.logs_dir,
    ]
    for mount in necessary_mounts:
        if mount:
            container_start_cmd.append('--mount')
            container_start_cmd.append(str(mount))
    container_start_cmd.append(clp_config.execution_container)

    scheduler_cmd = [
        'python3', '-u', '-m',
        'compression_job_handler.main',
        '--target-archive-size', str(clp_config.archive_output.target_archive_size),
        '--target-archive-dictionaries-data-size',
        str(clp_config.archive_output.target_dictionaries_size),
        '--target-encoded-file-size', str(clp_config.archive_output.target_encoded_file_size),
        '--target-segment-size', str(clp_config.archive_output.target_segment_size),
        '--no-progress-reporting',
        # V0.5 TODO: pass those variables using clp config
        '--db-uri', f"{clp_config.results_cache.get_uri()}/metadata-db",
        '--clp-db-host', clp_config.database.host,
        '--clp-db-port', str(clp_config.database.port),
        'fs',
        '--archives-dir', str(container_clp_config.archive_output.directory)
    ]
    cmd = container_start_cmd + scheduler_cmd
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)
    logger.info(f"Started {component_name}.")


def start_search_scheduler(instance_id: str, clp_config: CLPConfig, container_clp_config: CLPConfig,
                           mounts: CLPDockerMounts):
    logger.info(f"Starting {SEARCH_SCHEDULER_COMPONENT_NAME}...")

    container_name = f'clp-{SEARCH_SCHEDULER_COMPONENT_NAME}-{instance_id}'
    if container_exists(container_name):
        logger.info(f"{SEARCH_SCHEDULER_COMPONENT_NAME} already running.")
        return

    container_config_filename = f'{container_name}.yml'
    container_config_file_path = clp_config.logs_directory / container_config_filename
    with open(container_config_file_path, 'w') as f:
        yaml.safe_dump(container_clp_config.dump_to_primitive_dict(), f)

    clp_site_packages_dir = CONTAINER_CLP_HOME / 'lib' / 'python3' / 'site-packages'
    container_start_cmd = [
        'docker', 'run',
        '-di',
        '--network', 'host',
        '-w', str(CONTAINER_CLP_HOME),
        '--rm',
        '--name', container_name,
        '-e', f'PYTHONPATH={clp_site_packages_dir}',
        '-e', f'BROKER_URL=amqp://'
              f'{container_clp_config.search_queue.username}:{container_clp_config.search_queue.password}@'
              f'{container_clp_config.search_queue.host}:{container_clp_config.search_queue.port}',
        '-e', f'RESULT_BACKEND=rpc://'
              f'{container_clp_config.search_queue.username}:{container_clp_config.search_queue.password}'
              f'@{container_clp_config.search_queue.host}:{container_clp_config.search_queue.port}',
        # V0.5 TODO: customization for filer
        #'-u', f'{os.getuid()}:{os.getgid()}',
        '-u', f'root:root',
        '--mount', str(mounts.clp_home),
    ]
    necessary_mounts = [
        mounts.logs_dir,
    ]
    for mount in necessary_mounts:
        if mount:
            container_start_cmd.append('--mount')
            container_start_cmd.append(str(mount))
    container_start_cmd.append(clp_config.execution_container)
    scheduler_cmd = [
        'python3', '-u', '-m',
        'job_orchestration.scheduler.search_scheduler',
        '--config', str(container_clp_config.logs_directory / container_config_filename),
    ]

    # for now, let's use what we have.
    cmd = container_start_cmd + scheduler_cmd
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)

    logger.info(f"Started {SEARCH_SCHEDULER_COMPONENT_NAME}.")


def start_compression_worker(instance_id: str, clp_config: CLPConfig, container_clp_config: CLPConfig,
                             num_cpus: int, mounts: CLPDockerMounts):
    compression_queue_config = container_clp_config.compression_queue
    celery_method = 'job_orchestration.executor.compression'
    celery_route = f"{QueueName.COMPRESSION}"
    start_worker(COMPRESSION_WORKER_COMPONENT_NAME, instance_id, clp_config, container_clp_config,
                 compression_queue_config, celery_method, celery_route, num_cpus, mounts)


def start_search_worker(instance_id: str, clp_config: CLPConfig, container_clp_config: CLPConfig,
                        num_cpus: int, mounts: CLPDockerMounts):
    compression_queue_config = container_clp_config.search_queue
    celery_method = 'job_orchestration.executor.search'
    celery_route = f"{QueueName.SEARCH}"
    start_worker(SEARCH_WORKER_COMPONENT_NAME, instance_id, clp_config, container_clp_config,
                 compression_queue_config, celery_method, celery_route, num_cpus, mounts)


def start_worker(component_name: str, instance_id: str, clp_config: CLPConfig,
                 container_clp_config: CLPConfig, queue_config: Queue, celery_method: str,
                 celery_route: str, num_cpus: int, mounts: CLPDockerMounts):
    logger.info(f"Starting {component_name}...")

    container_name = f'clp-{component_name}-{instance_id}'
    if container_exists(container_name):
        logger.info(f"{component_name} already running.")
        return

    validate_worker_config(clp_config)

    # Create necessary directories
    clp_config.archive_output.directory.mkdir(parents=True, exist_ok=True)

    clp_site_packages_dir = CONTAINER_CLP_HOME / 'lib' / 'python3' / 'site-packages'
    container_start_cmd = [
        'docker', 'run',
        '-di',
        '--network', 'host',
        '-w', str(CONTAINER_CLP_HOME),
        '--rm',
        '--name', container_name,
        '-e', f'PYTHONPATH={clp_site_packages_dir}',
        '-e', f'BROKER_URL=amqp://'
              f'{queue_config.username}:{queue_config.password}@'
              f'{queue_config.host}:{queue_config.port}',
        '-e', f'RESULT_BACKEND=rpc://'
              f'{queue_config.username}:{queue_config.password}'
              f'@{queue_config.host}:{queue_config.port}',
        '-e', f'CLP_HOME={CONTAINER_CLP_HOME}',
        '-e', f'CLP_DATA_DIR={container_clp_config.data_directory}',
        '-e', f'CLP_ARCHIVE_OUTPUT_DIR={container_clp_config.archive_output.directory}',
        '-e', f'CLP_LOGS_DIR={container_clp_config.logs_directory}',
        # V0.5 TODO: customization for filer
        #'-u', f'{os.getuid()}:{os.getgid()}',
        '-u', f'root:root',
        '--mount', str(mounts.clp_home),
    ]
    necessary_mounts = [
        mounts.data_dir,
        mounts.logs_dir,
        mounts.archives_output_dir,
        mounts.input_logs_dir,
    ]
    for mount in necessary_mounts:
        if mount:
            container_start_cmd.append('--mount')
            container_start_cmd.append(str(mount))
    container_start_cmd.append(clp_config.execution_container)

    worker_cmd = [
        str(clp_site_packages_dir / 'bin' / 'celery'),
        '-A',
        celery_method,
        'worker',
        '--concurrency', str(num_cpus),
        '--loglevel', 'WARNING',
        '-Q', celery_route,
        '-n', component_name,
    ]
    cmd = container_start_cmd + worker_cmd
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)

    logger.info(f"Started {component_name}.")


def start_webui(instance_id: str, clp_config: CLPConfig, mounts: CLPDockerMounts):
    component_name = WEBUI_COMPONENT_NAME

    logger.info(f"Starting {component_name}...")

    container_name = f'clp-{component_name}-{instance_id}'
    if container_exists(container_name):
        logger.info(f"{component_name} already running.")
        return

    validate_webui_config(clp_config)

    # Start container
    container_cmd = [
        'docker', 'run',
        '-d',
        '--network', 'host',
        # '--rm',
        '--name', container_name,
        '-u', f'{os.getuid()}:{os.getgid()}',
        '-e', f"MONGO_URL={clp_config.results_cache.get_uri()}",
        '-e', f"PORT={clp_config.webui.port}",
        '-e', f"ROOT_URL=http://{clp_config.webui.host}",
    ]
    necessary_mounts = [
        mounts.clp_home,
    ]
    for mount in necessary_mounts:
        if mount:
            container_cmd.append('--mount')
            container_cmd.append(str(mount))
    container_cmd.append(clp_config.execution_container)

    node_cmd = [
        str(CONTAINER_CLP_HOME / 'bin' / 'node'),
        str(CONTAINER_CLP_HOME / 'var' / 'www' / 'main.js')
    ]
    cmd = container_cmd + node_cmd
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)

    logger.info(f"Started {component_name}.")


def main(argv):
    default_config_file_path = clp_home / CLP_DEFAULT_CONFIG_FILE_RELATIVE_PATH

    args_parser = argparse.ArgumentParser(description="Starts CLP")
    args_parser.add_argument('--config', '-c', default=str(default_config_file_path),
                             help="CLP package configuration file.")

    # Constant
    DATABASE_COMPONENTS = "databases"
    CONTROLLER_COMPONENTS = "controllers"

    component_args_parser = args_parser.add_subparsers(dest='component_name')
    component_args_parser.add_parser(DB_COMPONENT_NAME)
    component_args_parser.add_parser(COMPRESSION_QUEUE_COMPONENT_NAME)
    component_args_parser.add_parser(RESULTS_CACHE_COMPONENT_NAME)
    component_args_parser.add_parser(SEARCH_QUEUE_COMPONENT_NAME)
    component_args_parser.add_parser(SEARCH_SCHEDULER_COMPONENT_NAME)
    component_args_parser.add_parser(COMPRESSION_JOB_HANDLER_COMPONENT_NAME)
    # need to think about how to specify the argument
    component_args_parser.add_parser(SEARCH_WORKER_COMPONENT_NAME)
    component_args_parser.add_parser(COMPRESSION_WORKER_COMPONENT_NAME)
    component_args_parser.add_parser(WEBUI_COMPONENT_NAME)
    # Shortcut for multiple components
    component_args_parser.add_parser(DATABASE_COMPONENTS)
    component_args_parser.add_parser(CONTROLLER_COMPONENTS)
    # V0.5 TODO properly handle num-cpus argument
    # worker_args_parser = component_args_parser.add_parser(COMPRESSION_WORKER_COMPONENT_NAME)
    args_parser.add_argument('--num-cpus', type=int, default=0,
                             help="Number of logical CPU cores to use for compression")

    parsed_args = args_parser.parse_args(argv[1:])

    if parsed_args.component_name:
        component_name = parsed_args.component_name
    else:
        component_name = ""

    try:
        check_dependencies()
    except:
        logger.exception("Dependency checking failed.")
        return -1

    # Validate and load config file
    try:
        config_file_path = pathlib.Path(parsed_args.config)
        clp_config = validate_and_load_config_file(config_file_path, default_config_file_path, clp_home)

        # Validate and load necessary credentials
        if component_name in ['', DB_COMPONENT_NAME, SEARCH_SCHEDULER_COMPONENT_NAME,
                              COMPRESSION_JOB_HANDLER_COMPONENT_NAME, DATABASE_COMPONENTS,
                              CONTROLLER_COMPONENTS]:
            validate_and_load_db_credentials_file(clp_config, clp_home, True)
        if component_name in [
            '',
            COMPRESSION_QUEUE_COMPONENT_NAME,
            SEARCH_SCHEDULER_COMPONENT_NAME,
            SEARCH_QUEUE_COMPONENT_NAME,
            COMPRESSION_JOB_HANDLER_COMPONENT_NAME,
            COMPRESSION_WORKER_COMPONENT_NAME,
            SEARCH_WORKER_COMPONENT_NAME,
            CONTROLLER_COMPONENTS
        ]:
            # TODO Validate the search queue credentials when the search queue is actually used
            validate_and_load_compression_queue_credentials_file(clp_config, clp_home, True)

        clp_config.validate_data_dir()
        clp_config.validate_logs_dir()
    except:
        logger.exception("Failed to load config.")
        return -1

    # Get the number of CPU cores to use
    num_cpus = multiprocessing.cpu_count()
    if '' == component_name:
        if parsed_args.num_cpus != 0:
            num_cpus = parsed_args.num_cpus
        num_cpus = int(num_cpus / 2)

    if COMPRESSION_WORKER_COMPONENT_NAME == component_name and parsed_args.num_cpus != 0:
        num_cpus = parsed_args.num_cpus
    if SEARCH_WORKER_COMPONENT_NAME == component_name and parsed_args.num_cpus != 0:
        num_cpus = parsed_args.num_cpus

    container_clp_config, mounts = generate_container_config(clp_config, clp_home)

    # Create necessary directories
    clp_config.data_directory.mkdir(parents=True, exist_ok=True)
    clp_config.logs_directory.mkdir(parents=True, exist_ok=True)

    try:
        # Create instance-id file
        instance_id_file_path = clp_config.logs_directory / 'instance-id'
        if instance_id_file_path.exists():
            with open(instance_id_file_path, 'r') as f:
                instance_id = f.readline()
        else:
            instance_id = str(uuid.uuid4())[-4:]
            with open(instance_id_file_path, 'w') as f:
                f.write(instance_id)
                f.flush()

        conf_dir = clp_home / 'etc'

        # Start components
        if component_name in ['', DB_COMPONENT_NAME, DATABASE_COMPONENTS]:
            start_db(instance_id, clp_config, conf_dir)
            create_db_tables(instance_id, clp_config, container_clp_config, mounts)
        if component_name in ['', RESULTS_CACHE_COMPONENT_NAME, DATABASE_COMPONENTS]:
            start_results_cache(instance_id, clp_config, conf_dir)
        if component_name in ['', COMPRESSION_QUEUE_COMPONENT_NAME, CONTROLLER_COMPONENTS]:
            start_compression_queue(instance_id, clp_config)
        if component_name in ['', SEARCH_QUEUE_COMPONENT_NAME, CONTROLLER_COMPONENTS]:
            start_search_queue(instance_id, clp_config)
        if component_name in ['', COMPRESSION_JOB_HANDLER_COMPONENT_NAME, CONTROLLER_COMPONENTS]:
            start_compression_job_handler(instance_id, clp_config, container_clp_config, mounts)
        if component_name in ['', SEARCH_SCHEDULER_COMPONENT_NAME, CONTROLLER_COMPONENTS]:
            start_search_scheduler(instance_id, clp_config, container_clp_config, mounts)
        if component_name in ['', SEARCH_WORKER_COMPONENT_NAME]:
            start_search_worker(instance_id, clp_config, container_clp_config, num_cpus, mounts)
        if component_name in ['', COMPRESSION_WORKER_COMPONENT_NAME]:
            start_compression_worker(instance_id, clp_config, container_clp_config, num_cpus, mounts)
        if component_name in ['', WEBUI_COMPONENT_NAME]:
            start_webui(instance_id, clp_config, mounts)
    except Exception as ex:
        # Stop CLP
        subprocess.run(['python3', str(clp_home / 'sbin' / 'stop-clp')], check=True)

        if type(ex) == ValueError:
            logger.error(f"Failed to start CLP: {ex}")
        else:
            logger.exception("Failed to start CLP.")
        return -1

    return 0


if '__main__' == __name__:
    sys.exit(main(sys.argv))
